{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x151520c10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import plotly.express as px\n",
    "# import plotly.io as pio\n",
    "# import plotly.graph_objects as go\n",
    "# pio.renderers.default = \"notebook_connected\" # or use \"browser\" if you want plots to open with browser\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "from typing import List, Optional, Callable, Tuple, Union\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.utils import shuffle\n",
    "import json\n",
    "\n",
    "\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import utils, HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "\n",
    "# Saves computation time, since we don't need it for the contents of this notebook\n",
    "t.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.feature_detection.experiment import feature_detection_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "gpt2_small = HookedTransformer.from_pretrained(\"gpt2-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the baseline dataset\n",
    "\n",
    "def read_all_text_files(directory):\n",
    "    # List to hold the contents of all files\n",
    "    contents_list = []\n",
    "\n",
    "    # List all files in directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Check if file is a text file\n",
    "        if filename.endswith('.txt'):\n",
    "            # Construct full file path\n",
    "            filepath = os.path.join(directory, filename)\n",
    "\n",
    "            # Open the file and read the contents\n",
    "            with open(filepath, 'r') as f:\n",
    "                contents = f.read()\n",
    "\n",
    "            # Add the file contents to the list\n",
    "            contents_list.append(contents)\n",
    "\n",
    "    return contents_list\n",
    "\n",
    "training_subset = read_all_text_files('datasets/urlsf_subset01-1_data') + read_all_text_files('datasets/urlsf_subset01-182_data')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "599"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_training_subset = training_subset[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = {}\n",
    "# Specify the file path\n",
    "file_path = 'datasets/fantasy_200.json'\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "  # Load the JSON data from the file\n",
    "  dataset_fantasy = json.load(file)\n",
    "\n",
    "  stories[\"fantasy\"] = dataset_fantasy\n",
    "\n",
    "# Specify the file path\n",
    "file_path = 'datasets/scifi_200.json'\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "  # Load the JSON data from the file\n",
    "  dataset_scifi = json.load(file)\n",
    "\n",
    "  stories[\"scifi\"] = dataset_scifi\n",
    "\n",
    "# Specify the file path\n",
    "file_path = 'datasets/sports_200.json'\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "  # Load the JSON data from the file\n",
    "  dataset_sports = json.load(file)\n",
    "\n",
    "  stories[\"sports\"] = dataset_sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the year 2054, on a remote planet millions of light-years away, a team of astrophysicists stumbled upon an incredible phenomenon: a tear in the fabric of space-time. As they cautiously approached, a blinding light engulfed them, transporting the team to an alternate reality. They soon realized that in this dimension, humans possessed unimaginable powers. Determined to protect their discovery, they formed an intergalactic alliance, safeguarding the portal from those seeking to exploit it. As they continued their exploration, they discovered parallel universes teeming with life, sparking infinite possibilities for the future of humanity.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_scifi[79]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m baseline_dataset \u001b[39m=\u001b[39m small_training_subset\n\u001b[1;32m      3\u001b[0m evaluation_dataset \u001b[39m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfantasy\u001b[39m\u001b[39m\"\u001b[39m: stories[\u001b[39m\"\u001b[39m\u001b[39mfantasy\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m100\u001b[39m:\u001b[39m200\u001b[39m],\n\u001b[1;32m      5\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mscifi\u001b[39m\u001b[39m\"\u001b[39m: stories[\u001b[39m\"\u001b[39m\u001b[39mscifi\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m100\u001b[39m:\u001b[39m200\u001b[39m],\n\u001b[1;32m      6\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msports\u001b[39m\u001b[39m\"\u001b[39m: stories[\u001b[39m\"\u001b[39m\u001b[39msports\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m100\u001b[39m:\u001b[39m200\u001b[39m]\n\u001b[1;32m      7\u001b[0m }\n\u001b[0;32m----> 9\u001b[0m feature_detection_experiment(\n\u001b[1;32m     10\u001b[0m     gpt2_small,\n\u001b[1;32m     11\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mblocks.7.hook_resid_post\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m     feature_dataset,\n\u001b[1;32m     13\u001b[0m     baseline_dataset,\n\u001b[1;32m     14\u001b[0m     \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     15\u001b[0m     evaluation_dataset,\n\u001b[1;32m     16\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mdot\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     17\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mzero\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     18\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Imperial-2022/Dissertation/Transformer-Masking/src/feature_detection/experiment.py:47\u001b[0m, in \u001b[0;36mfeature_detection_experiment\u001b[0;34m(model, location, feature_dataset, baseline_dataset, use_all_activations, evaluation_dataset, inner_product_type, threshold_type)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39mPerforms an experiment to see how well inner products with the vector arising from the\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39mfeature dataset can be used to predict the labels of the evaluation dataset.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m# Get the feature vector from the feature dataset\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39m# By default, have this be the vector from the centre of the baseline\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m# activations to the centre of the feature activations\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m feature_vector \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39;49mcomparing_centres\u001b[39m.\u001b[39;49mfind_activations_centre_diff(\n\u001b[1;32m     48\u001b[0m     model,\n\u001b[1;32m     49\u001b[0m     feature_dataset,\n\u001b[1;32m     50\u001b[0m     baseline_dataset,\n\u001b[1;32m     51\u001b[0m     location,\n\u001b[1;32m     52\u001b[0m     \u001b[39m2\u001b[39;49m,\n\u001b[1;32m     53\u001b[0m     use_all_activations\n\u001b[1;32m     54\u001b[0m )\n\u001b[1;32m     55\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mpart 1 done\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[39m# Get the activations for the evaluation dataset\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Imperial-2022/Dissertation/Transformer-Masking/src/comparing_centres.py:68\u001b[0m, in \u001b[0;36mfind_activations_centre_diff\u001b[0;34m(model, target_dataset, baseline_dataset, location, max_batch_size, use_all_activations)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_activations_centre_diff\u001b[39m(\n\u001b[1;32m     54\u001b[0m   model: HookedTransformer,\n\u001b[1;32m     55\u001b[0m   target_dataset: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m   use_all_activations: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     60\u001b[0m ):\n\u001b[1;32m     61\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39m  Find the centre of the activations of the baseline dataset,\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39m  take this away from the centre of the activations of a second dataset.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m  Return the resulting difference vector\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m   baseline_centre \u001b[39m=\u001b[39m find_activations_centre(\n\u001b[1;32m     69\u001b[0m     model,\n\u001b[1;32m     70\u001b[0m     baseline_dataset,\n\u001b[1;32m     71\u001b[0m     location,\n\u001b[1;32m     72\u001b[0m     max_batch_size,\n\u001b[1;32m     73\u001b[0m     use_all_activations\n\u001b[1;32m     74\u001b[0m   )\n\u001b[1;32m     76\u001b[0m   baseline_target \u001b[39m=\u001b[39m find_activations_centre(\n\u001b[1;32m     77\u001b[0m     model,\n\u001b[1;32m     78\u001b[0m     target_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m     use_all_activations\n\u001b[1;32m     82\u001b[0m   )\n\u001b[1;32m     84\u001b[0m   difference \u001b[39m=\u001b[39m baseline_target \u001b[39m-\u001b[39m baseline_centre\n",
      "File \u001b[0;32m~/Documents/Imperial-2022/Dissertation/Transformer-Masking/src/comparing_centres.py:39\u001b[0m, in \u001b[0;36mfind_activations_centre\u001b[0;34m(model, dataset, location, max_batch_size, use_all_activations)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_activations_centre\u001b[39m(\n\u001b[1;32m     29\u001b[0m   model: HookedTransformer,\n\u001b[1;32m     30\u001b[0m   dataset: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m   use_all_activations: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     34\u001b[0m ):\n\u001b[1;32m     35\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m  Find the centre of the activations of a dataset, at some\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39m  layer of a certain model.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m   all_activations \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mdataset_activations_optimised_new(\n\u001b[1;32m     40\u001b[0m     model,\n\u001b[1;32m     41\u001b[0m     dataset,\n\u001b[1;32m     42\u001b[0m     location,\n\u001b[1;32m     43\u001b[0m     max_batch_size,\n\u001b[1;32m     44\u001b[0m     use_all_activations\n\u001b[1;32m     45\u001b[0m   )\n\u001b[1;32m     47\u001b[0m   \u001b[39m# Find the mean\u001b[39;00m\n\u001b[1;32m     48\u001b[0m   mean \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mmean(all_activations, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Imperial-2022/Dissertation/Transformer-Masking/src/utils.py:195\u001b[0m, in \u001b[0;36mdataset_activations_optimised_new\u001b[0;34m(model, dataset, location, max_batch_size, use_all_activations)\u001b[0m\n\u001b[1;32m    191\u001b[0m final_indices \u001b[39m=\u001b[39m final_indices\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[39m# print(batch_tokens)\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39m# Feed the tensor through the model\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m _, cache \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mrun_with_cache(batch_tokens, return_cache_object\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, remove_batch_dim\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    196\u001b[0m activations \u001b[39m=\u001b[39m cache[location]\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m use_all_activations:\n",
      "File \u001b[0;32m~/Documents/Imperial-2022/Dissertation/Transformer-Masking/tran-mask/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:394\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_with_cache\u001b[39m(\n\u001b[1;32m    381\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mmodel_args, return_cache_object\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, remove_batch_dim\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    382\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    389\u001b[0m     Union[ActivationCache, Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor]],\n\u001b[1;32m    390\u001b[0m ]:\n\u001b[1;32m    391\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[39m    Wrapper around run_with_cache in HookedRootModule. If return_cache_object is True, this will return an ActivationCache object, with a bunch of useful HookedTransformer specific methods, otherwise it will return a dictionary of activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 394\u001b[0m     out, cache_dict \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun_with_cache(\n\u001b[1;32m    395\u001b[0m         \u001b[39m*\u001b[39;49mmodel_args, remove_batch_dim\u001b[39m=\u001b[39;49mremove_batch_dim, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    396\u001b[0m     )\n\u001b[1;32m    397\u001b[0m     \u001b[39mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    398\u001b[0m         cache \u001b[39m=\u001b[39m ActivationCache(\n\u001b[1;32m    399\u001b[0m             cache_dict, \u001b[39mself\u001b[39m, has_batch_dim\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m remove_batch_dim\n\u001b[1;32m    400\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/Imperial-2022/Dissertation/Transformer-Masking/tran-mask/lib/python3.11/site-packages/transformer_lens/hook_points.py:363\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m cache_dict, fwd, bwd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    359\u001b[0m     names_filter, incl_bwd, device, remove_batch_dim\u001b[39m=\u001b[39mremove_batch_dim\n\u001b[1;32m    360\u001b[0m )\n\u001b[1;32m    362\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhooks(fwd_hooks\u001b[39m=\u001b[39mfwd, bwd_hooks\u001b[39m=\u001b[39mbwd, reset_hooks_end\u001b[39m=\u001b[39mreset_hooks_end, clear_contexts\u001b[39m=\u001b[39mclear_contexts):\n\u001b[0;32m--> 363\u001b[0m     model_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m    364\u001b[0m     \u001b[39mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    365\u001b[0m         model_out\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Documents/Imperial-2022/Dissertation/Transformer-Masking/tran-mask/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Imperial-2022/Dissertation/Transformer-Masking/tran-mask/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:325\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m shortformer_pos_embed\u001b[39m.\u001b[39mto(\n\u001b[1;32m    322\u001b[0m             devices\u001b[39m.\u001b[39mget_device_for_block_index(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg)\n\u001b[1;32m    323\u001b[0m         )\n\u001b[0;32m--> 325\u001b[0m     residual \u001b[39m=\u001b[39m block(\n\u001b[1;32m    326\u001b[0m         residual,\n\u001b[1;32m    327\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache[i]\n\u001b[1;32m    328\u001b[0m         \u001b[39mif\u001b[39;49;00m past_kv_cache \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    329\u001b[0m         \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each block\u001b[39;49;00m\n\u001b[1;32m    330\u001b[0m         shortformer_pos_embed\u001b[39m=\u001b[39;49mshortformer_pos_embed,\n\u001b[1;32m    331\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m stop_at_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[39m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    335\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Imperial-2022/Dissertation/Transformer-Masking/tran-mask/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Imperial-2022/Dissertation/Transformer-Masking/tran-mask/lib/python3.11/site-packages/transformer_lens/components.py:778\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    773\u001b[0m     resid_mid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_resid_mid(\n\u001b[1;32m    774\u001b[0m         resid_pre \u001b[39m+\u001b[39m attn_out\n\u001b[1;32m    775\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    776\u001b[0m     normalized_resid_mid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln2(resid_mid)\n\u001b[1;32m    777\u001b[0m     mlp_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_mlp_out(\n\u001b[0;32m--> 778\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(normalized_resid_mid)\n\u001b[1;32m    779\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     resid_post \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_resid_post(\n\u001b[1;32m    781\u001b[0m         resid_mid \u001b[39m+\u001b[39m mlp_out\n\u001b[1;32m    782\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mparallel_attn_mlp:\n\u001b[1;32m    784\u001b[0m     \u001b[39m# Dumb thing done by GPT-J, both MLP and Attn read from resid_pre and write to resid_post, no resid_mid used.\u001b[39;00m\n\u001b[1;32m    785\u001b[0m     \u001b[39m# In GPT-J, LN1 and LN2 are tied, in GPT-NeoX they aren't.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Imperial-2022/Dissertation/Transformer-Masking/tran-mask/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Imperial-2022/Dissertation/Transformer-Masking/tran-mask/lib/python3.11/site-packages/transformer_lens/components.py:593\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    589\u001b[0m     \u001b[39mself\u001b[39m, x: Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    590\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    591\u001b[0m     \u001b[39m# Technically, all these einsums could be done with a single matmul, but this is more readable.\u001b[39;00m\n\u001b[1;32m    592\u001b[0m     pre_act \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_pre(\n\u001b[0;32m--> 593\u001b[0m         einsum(\u001b[39m\"\u001b[39;49m\u001b[39mbatch pos d_model, d_model d_mlp -> batch pos d_mlp\u001b[39;49m\u001b[39m\"\u001b[39;49m, x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW_in)\n\u001b[1;32m    594\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_in\n\u001b[1;32m    595\u001b[0m     )  \u001b[39m# [batch, pos, d_mlp]\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mact_fn\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m_ln\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    597\u001b[0m         post_act \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_post(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_fn(pre_act))  \u001b[39m# [batch, pos, d_mlp]\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Imperial-2022/Dissertation/Transformer-Masking/tran-mask/lib/python3.11/site-packages/fancy_einsum/__init__.py:136\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    134\u001b[0m backend \u001b[39m=\u001b[39m get_backend(operands[\u001b[39m0\u001b[39m])\n\u001b[1;32m    135\u001b[0m new_equation \u001b[39m=\u001b[39m convert_equation(equation)\n\u001b[0;32m--> 136\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39;49meinsum(new_equation, \u001b[39m*\u001b[39;49moperands)\n",
      "File \u001b[0;32m~/Documents/Imperial-2022/Dissertation/Transformer-Masking/tran-mask/lib/python3.11/site-packages/fancy_einsum/__init__.py:54\u001b[0m, in \u001b[0;36mTorchBackend.einsum\u001b[0;34m(self, equation, *operands)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meinsum\u001b[39m(\u001b[39mself\u001b[39m, equation, \u001b[39m*\u001b[39moperands):\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtorch\u001b[39m.\u001b[39;49meinsum(equation, \u001b[39m*\u001b[39;49moperands)\n",
      "File \u001b[0;32m~/Documents/Imperial-2022/Dissertation/Transformer-Masking/tran-mask/lib/python3.11/site-packages/torch/functional.py:378\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39m_operands)\n\u001b[1;32m    375\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(operands) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m opt_einsum\u001b[39m.\u001b[39menabled:\n\u001b[1;32m    376\u001b[0m     \u001b[39m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[39m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m     \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49meinsum(equation, operands)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    380\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m opt_einsum\u001b[39m.\u001b[39mis_available():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "feature_dataset = stories[\"fantasy\"][:100] \n",
    "baseline_dataset = small_training_subset\n",
    "evaluation_dataset = {\n",
    "    \"fantasy\": stories[\"fantasy\"][100:200],\n",
    "    \"scifi\": stories[\"scifi\"][100:200],\n",
    "    \"sports\": stories[\"sports\"][100:200]\n",
    "}\n",
    "\n",
    "feature_detection_experiment(\n",
    "    gpt2_small,\n",
    "    \"blocks.7.hook_resid_post\",\n",
    "    feature_dataset,\n",
    "    baseline_dataset,\n",
    "    False,\n",
    "    evaluation_dataset,\n",
    "    \"dot\",\n",
    "    \"zero\"\n",
    ")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tran-mask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
