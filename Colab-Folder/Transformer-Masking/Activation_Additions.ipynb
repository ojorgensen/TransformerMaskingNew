{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPwbAZSIXRZqw/W6B+KaL9A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"ri6dss0LjTVh"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-xfNxzw8iU3B","executionInfo":{"status":"ok","timestamp":1688993354299,"user_tz":-60,"elapsed":13583,"user":{"displayName":"Ole Jorgensen","userId":"05748662968976220828"}},"outputId":"ae8ccee3-f451-4edf-e93a-2b54a5b3297e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running as a Colab notebook\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/MyDrive/AI-ML-Stuff/Dissertation/work/Transformer-Masking\n","Requirement already satisfied: transformer_lens in /usr/local/lib/python3.10/dist-packages (1.3.0)\n","Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.13.1)\n","Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.6.1)\n","Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.0.3)\n","Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.2.20)\n","Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (1.25.1)\n","Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (1.5.3)\n","Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (13.4.2)\n","Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.0.1+cu118)\n","Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.65.0)\n","Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.30.2)\n","Requirement already satisfied: typeguard<4.0.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (3.0.2)\n","Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.15.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (9.0.0)\n","Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.3.6)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2.27.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.2.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.70.14)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.8.4)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.16.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.10/dist-packages (from jaxtyping>=0.2.11->transformer_lens) (4.6.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2022.7.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (2.14.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10->transformer_lens) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10->transformer_lens) (16.0.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->transformer_lens) (2022.10.31)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->transformer_lens) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->transformer_lens) (0.3.1)\n","Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.3)\n","Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.31)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (5.9.5)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (1.28.0)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n","Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (0.1.2)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (1.3.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.20.3)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.16.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.0.12)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.10)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2023.5.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->transformer_lens) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->transformer_lens) (1.3.0)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.1)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n"]}],"source":["try:\n","  import google.colab\n","  IN_COLAB = True\n","  print(\"Running as a Colab notebook\")\n","  from google.colab import drive\n","  drive.mount('/content/gdrive')\n","  %cd /content/gdrive/MyDrive/AI-ML-Stuff/Dissertation/work/Transformer-Masking\n","  # Note: this is different to the usual setup, since I'm using\n","  # Alex Turner's activation addition repo\n","  commit = \"08efeb9\" # Stable commit\n","  get_ipython().run_line_magic(magic_name='pip', line=f'install -U git+https://github.com/montemac/algebraic_value_editing.git@{commit}')\n","except:\n","  IN_COLAB = False\n","  print(\"Running as a Jupyter notebook - intended for development only!\")"]},{"cell_type":"code","source":["import plotly.express as px\n","import plotly.io as pio\n","import plotly.graph_objects as go\n","pio.renderers.default = \"notebook_connected\" # or use \"browser\" if you want plots to open with browser\n","import torch as t\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import einops\n","from fancy_einsum import einsum\n","from typing import List, Optional, Callable, Tuple, Union, Dict\n","from functools import partial\n","from tqdm import tqdm\n","from IPython.display import display\n","import random\n","import os\n","import matplotlib.pyplot as plt\n","from sklearn.manifold import TSNE\n","from sklearn.utils import shuffle\n","import json\n","\n","# Neel's Stuff\n","from transformer_lens.hook_points import HookPoint\n","from transformer_lens import utils, HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n","\n","# Activation Addition Stuff\n","from algebraic_value_editing.completion_utils import print_n_comparisons\n","\n","# Saves computation time, since we don't need it for the contents of this notebook\n","t.set_grad_enabled(False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pCTxCEyOiebl","executionInfo":{"status":"ok","timestamp":1688993367704,"user_tz":-60,"elapsed":7677,"user":{"displayName":"Ole Jorgensen","userId":"05748662968976220828"}},"outputId":"9a475f84-4cc9-423a-ee21-e3aa679213a0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.autograd.grad_mode.set_grad_enabled at 0x7f966c7a8f10>"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"EMeYNlLRBjKO","executionInfo":{"status":"ok","timestamp":1688661083574,"user_tz":-60,"elapsed":40,"user":{"displayName":"Ole Jorgensen","userId":"05748662968976220828"}},"outputId":"0b357866-a3de-4fb5-cb88-2bef92dbfae7","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Jul  6 16:31:19 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P0    38W / 300W |  16146MiB / 16384MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["gpt2_small = HookedTransformer.from_pretrained(\"gpt2-small\").cuda()\n","gpt2_small.name = \"gpt2 small\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":431,"referenced_widgets":["c91d20a6fec54cb3ac6255ad47e2cc58","86ec96b7126949378373900d771f9efb","87953186b52249fa960bdccb193ba10d","fc93701a842b4b1eb2faa860ca79631d","1129ef3e9d164a9794c7aaf9615b6587","fb1384f4117f4300855f84b81fa9d5ae","9f3a4ace84924a40951d12227839201a","8f672533e94b48b7b25c3c6fa727b1fb","bea4844b64174f45b19f164ded0697c3","284418326edc4da388f0d7666ee5d675","1330a41cef5946758a1f1385bd1b291f","51940815eb9b4a87b032511bb9a17a73","f9190019d05a464290d385314b915586","62348efbdd3c427d9d1f591a2333ed64","4e507a4ca5b04b6b880e9f20093ac835","45e63b92fd71443383005aa3e2431100","714ee666b4ba49449d8289f7e65672bb","621c9c9c8d1244248709b52c743a011d","9bd185496b554837af324360c0279dd3","944c184af2a44e2ebe24d88921d2433b","22a29b9f533d4767b986ce0b83c6ab73","65561709bdbb497ca57c008c23095609","9a13af2219bf4d1ab0c33bf1203db406","ca1839de08c74e71ba06482f1cf30968","aaa90ad334d846de9cfd4a77292c61fa","df46ea9cdc584f29b8dcde87ce1e364e","af99a9c92433445e8b2d0dfeea819537","e6da1e9b880c4c098827882de15855bc","627110a8ca624a328180fa5a23a23469","4d4e0520e17f48989aae34c62ff9ff7b","51198489fafe4079857a6b813701dc2f","6112d611c2264e0f917e64a7ded263f7","46c7d1e79cce45afa828faa5d02e2fa7","aefcd9d83c064219b6b1241981628456","59470d0069174e3baf7409e1ecc07a79","b42d67dffa5a4bbeba6c5d1efaf4378c","8fdc8780afd14ef2b4de489a1332d4b2","b32715e36e5e455ca4adf82bf2c811d6","7de55863c96f4f4188cb12d8c8db4a24","34a1f7688b8f4df0b2033ee13b711d81","07c8f174c7f34c7cadd1a82314c81f40","60908e4810ce4e6c8b591c8d6305dbd7","90e67b9c84a24256936ba46d1dd5668e","4c5f60c6ef7a486e934e38052e4a3606","90c80c0f59604b09bb14e0870314a705","7db23d2c77db466fbe5a4aa880849810","1e461f2e3db14e38a6cd8209ff53a56d","62b1f89646bb45dca6310e0ec6144d60","e3737af6b3934a938c61c74bfa4b57dd","e0ccf269f61d40bb994852a7d8353311","729d466388ff45428d492fc9899ae0b4","8ca883e80c524ec592e0a25236311361","1e197f0c6ef94e11aa36d1f4bdf62edd","ecb8b8ef1a0045c1b4feaeb91e900223","9e729a8cd5c149cc92966933dcb7a8a3","394e00a616bb4e98bff821c79b92f823","0e2f62464a134d37bec549120365e46f","d165a135e6574d889653db32b38f4ac2","bafad76c8ba94fa2bb0cc6e783a67a8a","5d1bb81d27bb42c0b76270fe0c9e045c","0d47b395e9804cb6a91f2ada57318a22","f8f6e43b277f4ba7b184f15738a401d0","b3451e452f7045cd99e5a5eb41ebe6a9","02733c29576c4316a1226444757da8ee","b3a9ece780254b9d996debc5c04a336a","e82a963aa7464d34af81af1a6c8b3cc9"]},"id":"xG1RgbPwtDk5","executionInfo":{"status":"ok","timestamp":1688991249505,"user_tz":-60,"elapsed":15451,"user":{"displayName":"Ole Jorgensen","userId":"05748662968976220828"}},"outputId":"e3165b13-0159-4aa0-c256-13011b521033"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c91d20a6fec54cb3ac6255ad47e2cc58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51940815eb9b4a87b032511bb9a17a73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a13af2219bf4d1ab0c33bf1203db406"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aefcd9d83c064219b6b1241981628456"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90c80c0f59604b09bb14e0870314a705"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"394e00a616bb4e98bff821c79b92f823"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Using pad_token, but it is not set yet.\n"]},{"output_type":"stream","name":"stdout","text":["Loaded pretrained model gpt2-small into HookedTransformer\n","Moving model to device:  cuda\n"]}]},{"cell_type":"code","source":["gpt2_xl = HookedTransformer.from_pretrained(\"gpt2-xl\").cuda()\n","gpt2_xl.name = \"gpt2 xl\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":431,"referenced_widgets":["e200561bd51f4e8eae309e0b6e404d73","419a3e5060994d5190b4d679713cdfb2","f94fa1f60dd24e37992b5b4b130d7a03","f5abdfbcfed54f25a2826d95c307f2ad","4c542604fbb949058b149fe7726e2c78","c0d7d27051ab4c0dadcc2850597edb7a","0d59ad04150a48d8acf69cc2058911d4","34461d6c686a4548a1ca6ef3a0245ea1","1d0aecbf1dc541c29dbc7c8ea7dc920e","53bf84337c7f464eb3afb1a6b1267460","b4b70d1d9fe247859cc639c710b446bf","10c4eae272974f139797b444d246b92a","afbbd2b5156547058993da8ce8deb16a","6993802c86314751881db225fa54bb4e","a7d98ea1a28f42e3964fc4afa9fc2627","02c35ec3cc814581a7b5f260ee8cb1ea","612f6ebf22c044138d4461455621f31f","2ac9a96ce6b440728e87d41a015e2554","d95b03dcf13746ec921ef72151aca5ce","7f730d304353439cb399e195f1d27fcd","365834126f2a4be7bbe5fa1019e31175","bb1c95ea3b9a4f6f87fa1bbddf466991","18964ad58042480a9c1431f30d781afd","3d18191522764221bfc323196532b174","e53c04b4f01a4e66a608d659be71d647","a1c69a2bcd1b48d480c4568cd85549c2","7fa69cc9c8d74e2289f4f390304aa41a","427065ca53464832b9d45c152bc6aa7b","97b674a57daa462bae3d7fa237ddd080","5134c87f2a8b4b49a76855f703556831","1e8ec92d1f6c4a15a5f36cb6e93bb955","782e4429d77741cb96ad63b2450e9fec","26af6b55fa6a41eda2878dd383098cca","c5efcad68a204df8965a291da62ecd8f","8445e6ea56114d6babec4fd1abaa8065","3ed9df44da734fc793eb48253afda2fa","c63bf7b645d24ab8bb0945c1e8a21db0","a4953c3c44144346ac2021fca87bc4b6","bb2abc5f03fb41aeb35a14a5dc2f8314","7b41849075b040cb8b081eae96667a53","eb57c04344244569bb3b00711d70286b","4dcf60ee47904885bab8190b06b603f8","4cb515b6ad4d468589822062c776608f","f3adb2289a3749b59d79bb83f5d7e51d","a139510b512245b08e6420e7c78b912f","7ac86dcd3e3849c188f5ff63f48a5693","a1aaa71f5c91453588a880717a8356a5","e16ea878d3a44280ab1e2a72f86fc672","4c22bbcdd63e4789865c686b209ac42e","a74eb18443784e4b91196bb1abba0a81","398e25fd3d0842d8ab28f58b07e9ae84","6a9e5b3b9d7f44a89ec3ab74d54a7798","801afd7361884a91a032345faa20d173","0f46017202a0468f92b9eab9513c9897","897430b2e28d47eda798461c004455f1","d7a0999aa1b1428ab2ffa7738eec984a","33de5c2630d84eba8c3f9dfac0b77582","6d40a35346a44286b7c5e698c67d7b92","ed21a828bf4a482b9592bc254bd71815","2dee260a4ecf4716a680ba5ce97b56dc","74ae2bb4a905436d918dc7d443187d46","939288fd20c941b49525495d47244c36","a570f699e247488c81c23db3a31afd2b","f4a6a66fa9014968840c9d69de565d05","146b347539c74c9daf216c5805cc9cb3","1be5ea410332439cba95f7b76ac56cb5"]},"id":"-c4MBmAxihSp","executionInfo":{"status":"ok","timestamp":1688993475743,"user_tz":-60,"elapsed":69245,"user":{"displayName":"Ole Jorgensen","userId":"05748662968976220828"}},"outputId":"7e4c3f8f-198c-4cbb-ae89-f41b6a811317"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e200561bd51f4e8eae309e0b6e404d73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10c4eae272974f139797b444d246b92a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18964ad58042480a9c1431f30d781afd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5efcad68a204df8965a291da62ecd8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a139510b512245b08e6420e7c78b912f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7a0999aa1b1428ab2ffa7738eec984a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Using pad_token, but it is not set yet.\n"]},{"output_type":"stream","name":"stdout","text":["Loaded pretrained model gpt2-xl into HookedTransformer\n","Moving model to device:  cuda\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"jl-IC_zRl2Uh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Functions"],"metadata":{"id":"SlVL_7GuSinW"}},{"cell_type":"code","source":["def SVD(matrix: t.Tensor) -> Tuple[t.Tensor, t.Tensor, t.Tensor]:\n","  \"\"\"\n","  Compute the SVD of a matrix.\n","  Returns the three associated matrices\n","  \"\"\"\n","  U, S, V_H = t.linalg.svd(matrix)\n","  return U, S, V_H\n"],"metadata":{"id":"yNaOlaSowVxr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run a dataset through the model, store all the activations\n","def dataset_activations(\n","  model: HookedTransformer,\n","  dataset: List[str]\n","):\n","  # TODO: make this run in batches instead of at once.\n","  #\n","  # Tokenise the batch, form a batch tensor\n","  batch_tokens = model.to_tokens(dataset)\n","  # Feed the tensor through the model\n","  logits, cache = model.run_with_cache(batch_tokens, return_cache_object=True, remove_batch_dim=False)\n","\n","  return logits, cache"],"metadata":{"id":"mkL2d-nOyGTB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.nn.utils.rnn import pad_sequence\n","\n","def dataset_activations_optimised(\n","  model: HookedTransformer,\n","  dataset: List[str],\n","  location: str,\n","  max_batch_size: int\n","):\n","\n","  num_batches = (len(dataset) + max_batch_size - 1) // max_batch_size\n","  all_final_activations = []\n","\n","  # Process each batch\n","  for batch_idx in range(num_batches):\n","    t.cuda.empty_cache()\n","    # print(\"batch_idx be: \", batch_idx)\n","    # Determine the start and end index for this batch\n","    start_idx = batch_idx * max_batch_size\n","    end_idx = min(start_idx + max_batch_size, len(dataset))\n","\n","    # Extract the subset of the dataset for this batch\n","    batch_subset = dataset[start_idx:end_idx]\n","\n","    # Tokenise the batch, form a batch tensor\n","    batch_tokens = model.to_tokens(batch_subset)\n","\n","    mask = batch_tokens != 50256\n","    final_indices = ((mask.cumsum(dim=1) == mask.sum(dim=1).unsqueeze(1)).int()).argmax(dim=1)\n","    final_indices = final_indices.view(-1,1)\n","\n","    # print(batch_tokens)\n","    # Feed the tensor through the model\n","    _, cache = model.run_with_cache(batch_tokens, return_cache_object=True, remove_batch_dim=False)\n","    activations = cache[location]\n","\n","\n","    # # Take the last activation\n","    index_expanded = final_indices.unsqueeze(-1).expand(-1, -1, activations.size(2))\n","    # print(\"index_expanded: \", index_expanded)\n","    final_activations = t.gather(activations, 1, index_expanded)\n","    # Move the activations to the CPU and store them\n","    final_activations = final_activations.cpu()\n","    final_activations = final_activations.squeeze()\n","    all_final_activations.append(final_activations)\n","\n","  all_final_activations = t.cat(all_final_activations, dim=0)\n","\n","  # print(\"all final activations shape is: \", all_final_activations.shape)\n","\n","\n","  return all_final_activations"],"metadata":{"id":"w7ekmKKsiqLY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def dataset_activations_optimised_locations(\n","  model: HookedTransformer,\n","  dataset: List[str],\n","  layers: int,\n","  location: str,\n","  max_batch_size: int\n","):\n","  \"\"\"\n","  Same as earlier function, but returns activations for all locations.\n","  \"\"\"\n","\n","  num_batches = (len(dataset) + max_batch_size - 1) // max_batch_size\n","  all_final_activations = {}\n","\n","  # Process each batch\n","  for batch_idx in range(num_batches):\n","    t.cuda.empty_cache()\n","    # print(\"batch_idx be: \", batch_idx)\n","    # Determine the start and end index for this batch\n","    start_idx = batch_idx * max_batch_size\n","    end_idx = min(start_idx + max_batch_size, len(dataset))\n","\n","    # Extract the subset of the dataset for this batch\n","    batch_subset = dataset[start_idx:end_idx]\n","\n","    # Tokenise the batch, form a batch tensor\n","    batch_tokens = model.to_tokens(batch_subset)\n","\n","    mask = batch_tokens != 50256\n","    final_indices = ((mask.cumsum(dim=1) == mask.sum(dim=1).unsqueeze(1)).int()).argmax(dim=1)\n","    final_indices = final_indices.view(-1,1)\n","\n","    # print(batch_tokens)\n","    # Feed the tensor through the model\n","    _, cache = model.run_with_cache(batch_tokens, return_cache_object=True, remove_batch_dim=False)\n","\n","    for layer in range(layers):\n","      activations = cache[location.format(layer)]\n","      # # Take the last activation\n","      index_expanded = final_indices.unsqueeze(-1).expand(-1, -1, activations.size(2))\n","      # print(\"index_expanded: \", index_expanded)\n","      final_activations = t.gather(activations, 1, index_expanded)\n","      # Move the activations to the CPU and store them\n","      final_activations = final_activations.cpu()\n","      final_activations = final_activations.squeeze()\n","      all_final_activations.setdefault(layer, []).append(final_activations)\n","\n","\n","\n","  for layer in range(layers):\n","    all_final_activations[layer] = t.cat(all_final_activations[layer], dim=0)\n","\n","\n","  return all_final_activations"],"metadata":{"id":"4jnm3fIj2dry"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reshape into a matrix (looks at activations of each token)\n","\n","def reshape_activations(\n","  batch_activations: t.Tensor\n",") -> t.Tensor:\n","  squeezed_tensor = einops.rearrange(batch_activations, 'b tokens dim -> (b tokens) dim')\n","  return squeezed_tensor\n","\n"],"metadata":{"id":"JpEwtNm9bwAw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def activation_SVD(\n","    model: HookedTransformer,\n","    dataset: List[str],\n","    location: str\n",") -> Tuple[t.Tensor, t.Tensor, t.Tensor]:\n","  _, cache = dataset_activations(model, dataset)\n","  activation_cache = cache[location]\n","  squeezed_activations = reshape_activations(activation_cache)\n","  U, S, V_H = SVD(squeezed_activations)\n","  return U, S, V_H\n"],"metadata":{"id":"Wpo3P0WSeivc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run a dataset through the model, store all the activations\n","def dataset_activations_tokens(\n","  model: HookedTransformer,\n","  dataset_tokens: t.Tensor\n","):\n","  # Tokenise the batch, form a batch tensor\n","  batch_tokens = dataset_tokens\n","  # Feed the tensor through the model\n","  logits, cache = model.run_with_cache(batch_tokens, return_cache_object=True, remove_batch_dim=False)\n","  return logits, cache\n","\n","def reshape_activations(\n","  batch_activations: t.Tensor\n",") -> t.Tensor:\n","  squeezed_tensor = einops.rearrange(batch_activations, 'b tokens dim -> (b tokens) dim')\n","  return squeezed_tensor\n","\n","def activation_SVD_tokens(\n","    model: HookedTransformer,\n","    dataset_tokens: List[str],\n","    location: str\n",") -> Tuple[t.Tensor, t.Tensor, t.Tensor]:\n","  _, cache = dataset_activations_tokens(model, dataset_tokens)\n","  activation_cache = cache[location]\n","  squeezed_activations = reshape_activations(activation_cache)\n","  U, S, V_H = SVD(squeezed_activations)\n","  return U, S, V_H"],"metadata":{"id":"4VLRi_MTyer2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def activation_SVD_covariance(\n","    model: HookedTransformer,\n","    dataset_tokens: List[str],\n","    location: str\n",") -> Tuple[t.Tensor, t.Tensor, t.Tensor]:\n","  \"\"\"\n","  Similar to normal covariance, but we look at the normalised covariance matrix instead.\n","  \"\"\"\n","  _, cache = dataset_activations_tokens(model, dataset_tokens)\n","  activation_cache = cache[location]\n","  squeezed_activations = reshape_activations(activation_cache)\n","  print(squeezed_activations.shape)\n","  mean_activation = squeezed_activations.mean(dim=0, keepdim=True)\n","  centred_activations = squeezed_activations - mean_activation\n","  covariance_matrix = centred_activations.T @ centred_activations\n","  print(covariance_matrix.shape)\n","  U, S, V_H = SVD(covariance_matrix)\n","  return U, S, V_H\n","\n","\n","# activation_SVD_covariance(gpt2_small, multiplication_dataset, 'blocks.5.hook_attn_out')\n","\n"],"metadata":{"id":"hfjuPEui6nbG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Using tokens as a starting point, and also using the covariance matrix\n","def activation_SVD_tokens_covariance(\n","    model: HookedTransformer,\n","    dataset_tokens: List[str],\n","    location: str\n",") -> Tuple[t.Tensor, t.Tensor, t.Tensor]:\n","  _, cache = dataset_activations_tokens(model, dataset_tokens)\n","  activation_cache = cache[location]\n","  squeezed_activations = reshape_activations(activation_cache)\n","  mean_activation = squeezed_activations.mean(dim=0, keepdim=True)\n","  centred_activations = squeezed_activations - mean_activation\n","  covariance_matrix = centred_activations.T @ centred_activations\n","  U, S, V_H = SVD(covariance_matrix)\n","  return U, S, V_H"],"metadata":{"id":"-9gslURRIj4w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Singular Vector Functions"],"metadata":{"id":"BEpbuqhOjC4n"}},{"cell_type":"code","source":["def dataset_projection(\n","    X: t.Tensor,\n","    B: t.Tensor\n",") -> t.Tensor:\n","  \"\"\"\n","  Take in dataset X (with datapoints as rows) and an orthogonal basis B of a subspace.\n","\n","  The basis should be of dimension D x M, with M vectors in the basis each of dim D.\n","\n","  Compute the projection of each datapoint on this subspace, store the results as\n","  another dataset in the same form as the original.\n","  \"\"\"\n","\n","  return B.T @ B @ X.T\n"],"metadata":{"id":"wan-v3Hoi5_X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def top_k_projection(\n","    X: t.Tensor,\n","    V_H: t.Tensor,\n","    k: int\n",") -> t.Tensor:\n","  \"\"\"\n","  Project the dataset X onto the top k orthogonal basis vectors in V_H\n","  \"\"\"\n","  B = V_H[:k,:]\n","  proj = dataset_projection(X, B)\n","  return proj.T"],"metadata":{"id":"T0YuQu7yjJC4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def matrix_error(\n","    X_1: t.Tensor,\n","    X_2: t.Tensor\n",") -> int:\n","  \"\"\"\n","  Treat X_1 and X_2 as rows of datapoints.\n","  Then, take the difference between these matrices,\n","  compute the l_2 norm of each row,\n","  and then sum these norms.\n","\n","  Return the sum of these norms.\n","  \"\"\"\n","  X = X_1 - X_2\n","  norms = t.norm(X, dim=1, p=2)\n","  sum_of_norms = t.sum(norms)\n","\n","  return sum_of_norms"],"metadata":{"id":"MIQwJ_ZkquTx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"hW7eRlqZmBE1"}},{"cell_type":"markdown","source":["# PCA Reconstruction Errors"],"metadata":{"id":"V7EWUwzLj-gg"}},{"cell_type":"code","source":["def pca_reconstruction_errors(\n","    model: HookedTransformer,\n","    target_dataset: List[str],\n","    comparison_datasets: Dict[str, List[str]],\n","    k: int,\n","    layers: int,\n","    location: str\n","):\n","  \"\"\"\n","  Given a target dataset and a comparison dataset, with a transformer model,\n","  for each location in the model:\n","    1. Compute the singular value decomposition of activations of the final token\n","    of the model for the target dataset.\n","    2. Consider the subspace corresponding to the top k principle components. (determine k lmao)\n","    3. Project the activations corresponding to the comparison dataset onto this subspace.\n","    4. Look at the L_2 error of the reconstruction (as a fraction of the total length).\n","\n","  Repeat this for every location in the model, construct a plot for the MLP layers, the attention layers, and\n","  the residual stream.\n","  \"\"\"\n","  # Get the number of layers of the model\n","  # Shouldn't hardcode, but I know this for gpt2 xl is 48\n","  all_accuracies = {}\n","  all_comparison_activations_dict = {}\n","  target_activations_dict = dataset_activations_optimised_locations(\n","    model,\n","    target_dataset,\n","    layers,\n","    location,\n","    2\n","  )\n","  for name, comparison_dataset in comparison_datasets.items():\n","    comparison_activations_dict = dataset_activations_optimised_locations(\n","      model,\n","      comparison_dataset,\n","      layers,\n","      location,\n","      2\n","    )\n","\n","    all_comparison_activations_dict[name] = comparison_activations_dict\n","\n","    all_accuracies[name] = []\n","\n","  for layer in range(layers):\n","    target_activations = target_activations_dict[layer]\n","    # Do SVD\n","    _, _, V_H = SVD(target_activations)\n","\n","\n","    for name, comparison_dataset in comparison_datasets.items():\n","      comparison_activations_dict = all_comparison_activations_dict[name]\n","      comparison_activations = comparison_activations_dict[layer]\n","      # Project the comparison activations onto the top k vectors of V_H\n","      comparison_approx = top_k_projection(comparison_activations, V_H, k)\n","\n","      # Get the errors\n","      error = matrix_error(comparison_approx, comparison_activations)\n","      total_l2_norms = t.sum(t.norm(comparison_activations, dim=1, p=2))\n","      error_fraction = error / total_l2_norms\n","      all_accuracies[name].append(1 - error_fraction)\n","\n","  print(all_accuracies)\n","\n","  for name, accuracies in all_accuracies.items():\n","    plt.plot(accuracies, label=name)\n","  plt.ylabel(\"Reconstruction accuracies (as fraction of original)\")\n","  plt.xlabel(\"Layer\")\n","  plt.title(f\"Reconstruction accuracies of comparison projection on top {k} vectors of target activations at {location}\")\n","  plt.legend()\n","  plt.show()\n","\n","\n"],"metadata":{"id":"HU3owtZfjJ7I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pca_reconstruction_errors_self(\n","    model: HookedTransformer,\n","    datasets: Dict[str, List[str]],\n","    k: int,\n","    layers: int,\n","    location: str\n","):\n","  \"\"\"\n","  Given a target dataset and a comparison dataset, with a transformer model,\n","  for each location in the model:\n","    1. Compute the singular value decomposition of activations of the final token\n","    of the model for the target dataset.\n","    2. Consider the subspace corresponding to the top k principle components. (determine k lmao)\n","    3. Project the activations corresponding to the comparison dataset onto this subspace.\n","    4. Look at the L_2 error of the reconstruction (as a fraction of the total length).\n","\n","  Repeat this for every location in the model, construct a plot for the MLP layers, the attention layers, and\n","  the residual stream.\n","  \"\"\"\n","  # Get the number of layers of the model\n","  # Shouldn't hardcode, but I know this for gpt2 xl is 48\n","  all_accuracies = {}\n","  all_activations_dict = {}\n","\n","  for name, dataset in datasets.items():\n","    activations_dict = dataset_activations_optimised_locations(\n","      model,\n","      dataset,\n","      layers,\n","      location,\n","      2\n","    )\n","\n","    all_activations_dict[name] = activations_dict\n","\n","    all_accuracies[name] = []\n","\n","    for layer in range(layers):\n","        # do SVD\n","        activations = all_activations_dict[name][layer]\n","        _, _, V_H = SVD(activations)\n","\n","        # Project the comparison activations onto the top k vectors of V_H\n","        approx = top_k_projection(activations, V_H, k)\n","\n","        # Get the errors\n","        error = matrix_error(approx, activations)\n","        total_l2_norms = t.sum(t.norm(activations, dim=1, p=2))\n","        error_fraction = error / total_l2_norms\n","        all_accuracies[name].append(1 - error_fraction)\n","\n","        gc.collect()\n","        t.cuda.empty_cache()\n","\n","  print(all_accuracies)\n","\n","  for name, accuracies in all_accuracies.items():\n","    plt.plot(accuracies, label=name)\n","  plt.ylabel(\"Reconstruction accuracies (as fraction of original)\")\n","  plt.xlabel(\"Layer\")\n","  plt.title(f\"Reconstruction accuracies of projection on top {k} vectors of activations at {location}\")\n","  plt.legend()\n","  plt.show()"],"metadata":{"id":"As_1TpKDue8l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def activation_plot_final_acts_optimised(\n","  model: HookedTransformer,\n","  datasets_lang: List[str],\n","  datasets_tokens: List,\n","  plot_type: str,\n","  location: str,\n","  dimension: int,\n","  random: bool = False,\n","  centre: bool = True\n","):\n","  \"\"\"\n","  Given a dataset, create a plot of the activations for the final token.\n","  Use gpt 2 small, look at any given location.\n","  Works for either numerical or categorical labels.\n","  Should support both t-SNE and PCA.\n","  \"\"\"\n","  t.cuda.empty_cache()\n","  activation_dict = {}\n","  # Do the forward pass for each dataset\n","  for name in datasets_lang:\n","    dataset_lang = datasets_lang[name]\n","    final_activations = dataset_activations_optimised(model, dataset_lang, location, 2)\n","    # Looking at the final activations! Might want to change this?\n","\n","    # target_X = reshape_activations(activations)\n","\n","    # Convert the tensor to a numpy array as scikit-learn works with numpy arrays\n","    data_numpy = final_activations.cpu().numpy()\n","    activation_dict[name] = data_numpy\n","    print(\"data numpy shape is: \", data_numpy.shape)\n","\n","  for name in datasets_tokens:\n","    dataset_token = datasets_tokens[name]\n","    activations = dataset_activations_tokens(model, dataset_token)[1][location]\n","    # Looking at the final activations! Might want to change this?\n","    final_activations = activations[:,-1,:]\n","\n","    # target_X = reshape_activations(activations)\n","\n","    # Convert the tensor to a numpy array as scikit-learn works with numpy arrays\n","    data_numpy = final_activations.cpu().numpy()\n","    activation_dict[name] = data_numpy\n","\n","  all_data = []\n","  all_labels = []\n","  for label, data in activation_dict.items():\n","      all_data.append(data)\n","      all_labels.extend([label] * len(data))\n","\n","  all_data = np.concatenate(all_data, axis=0)\n","\n","  print(\"all data shape is: \", all_data.shape)\n","\n","  if random:\n","\n","    # Determine the number of data points per label (assuming all labels have the same number of data points)\n","    num_points_per_label = len(activation_dict[list(activation_dict.keys())[0]])\n","\n","    # Calculate the mean and variance of the entire dataset\n","    mean = np.mean(all_data, axis=0)\n","    variance = np.var(all_data, axis=0)\n","    # Generate some random data\n","    # Will use the same mean and variance as the data\n","    # Generate synthetic data with the same variance\n","    if centre:\n","      synthetic_data = np.random.normal(loc=mean, scale=np.sqrt(variance), size=(num_points_per_label, all_data.shape[1]))\n","    else:\n","      zeros = 0 * mean\n","      synthetic_data = np.random.normal(loc=zeros, scale=np.sqrt(variance), size=(num_points_per_label, all_data.shape[1]))\n","    synthetic_labels = ['Synthetic'] * len(synthetic_data)\n","\n","    # Concatenate the synthetic data with the original data\n","    all_data = np.concatenate([all_data, synthetic_data], axis=0)\n","    all_labels.extend(synthetic_labels)\n","\n","\n","\n","  # Initialize the t-SNE\n","  tsne = TSNE(n_components=2, random_state=21)\n","\n","  # Fit and transform the data to 2D\n","  data_2d = tsne.fit_transform(all_data)\n","\n","  data_2d_copy, all_labels_copy = data_2d, all_labels\n","\n","\n","  # Shuffle the data and labels\n","  data_2d, all_labels = shuffle(data_2d, all_labels, random_state=42)\n","\n","  # Plot the transformed data\n","  # Create a colormap for labels\n","  unique_labels = list(set(all_labels))\n","  colors = plt.cm.get_cmap('viridis', len(unique_labels))\n","\n","  # Plot the transformed data with labels\n","  plt.figure(figsize=(6, 5))\n","  markers = ['o', 's', '^', 'D', 'P']\n","\n","  # Plot all points together, coloring them based on their labels\n","  for i, label in enumerate(all_labels):\n","      color_idx = unique_labels.index(label)\n","      marker = markers[color_idx % len(markers)]\n","      plt.scatter(data_2d[i, 0], data_2d[i, 1], color=colors(color_idx), s=20, alpha=0.6, label=label if color_idx not in [unique_labels.index(lbl) for lbl in all_labels[:i]] else \"\", marker=marker)\n","\n","  # Add a legend\n","  handles, labels = plt.gca().get_legend_handles_labels()\n","  plt.legend(handles, labels, title=\"Labels\")\n","\n","  plt.xlabel('t-SNE feature 0')\n","  plt.ylabel('t-SNE feature 1')\n","  plt.title(f't-SNE visualization of the final activations of {model.name} at {location}')\n","  plt.show()\n","  return data_2d_copy, all_labels_copy"],"metadata":{"id":"P_gWc_J4yN0w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["addition_dataset = [str(x) + \" + \" + str(y) + \" =\" for x in range(24) for y in range(24)]\n","multiplication_dataset = [str(x) + \" * \" + str(y) + \" =\" for x in range(24) for y in range(24)]"],"metadata":{"id":"ju6TW52QsSFL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Activation Addition Utils"],"metadata":{"id":"uFR3y6sLmHW-"}},{"cell_type":"code","source":["class ActivationAddition:\n","  \"\"\"\n","  Specifies a Dataset, a coefficient, and a location in the model.\n","  Then provides a method for finding the principal component at this location,\n","  which can be used for activation addition interventions in the residual stream.\n","  Note: I'm going to be overwriting the version that currently exists for this.\n","  This is to allow me to do PCA stuff, not just use prompts.\n","  \"\"\"\n","\n","\n","  def __init__(\n","      self,\n","      coeff: float,\n","      act_name: Union[str, int],\n","      dataset: List[str],\n","  )\n","    \"\"\"\n","    Specifies a model location ('act_name') from which to extract activations.\n","    PCA will then be used to extract the top principal component\n","    of the activations of the final token of the dataset at this location.\n","    This will then be multiplied by 'coeff'.\n","    TODO: identify how big to make the vector as a baseline.\n","    \"\"\"\n","\n","    self.coeff = coeff\n","\n","\n","\n"],"metadata":{"id":"SbOiGBTznpIw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Activation Addition Intervention"],"metadata":{"id":"RKnEUHZCve4N"}},{"cell_type":"markdown","source":["My plan for this is to use the existing print_n_comparisons function from the algebraic_value_editing library. This should mean that I only need to change the ActivationAddition class, and might be able to just plug and play with the rest?\n","\n","Don't think this works - probably need to fork the repo and make my changes to it."],"metadata":{"id":"xb2KnZTlybdG"}},{"cell_type":"code","source":["def run_with_intervention(\n","    model: HookedTransformer,\n","    prompt: str,\n","    addition_vector: ActivationAddition,\n","    temperature: float,\n","    examples: int = 5,\n",")\n","  \"\"\"\n","  Complete a forward pass of a model, with an intervention on the\n","  residual stream corresponding to activation addition.\n","\n","  This activation vector, and where to intervene on the model, is contained\n","  in the addition_vector object.\n","  \"\"\""],"metadata":{"id":"DnMhIdW8mOkL"},"execution_count":null,"outputs":[]}]}