{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x29a0a6fd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import plotly.express as px\n",
    "# import plotly.io as pio\n",
    "# import plotly.graph_objects as go\n",
    "# pio.renderers.default = \"notebook_connected\" # or use \"browser\" if you want plots to open with browser\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "from typing import List, Optional, Callable, Tuple, Union\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.utils import shuffle\n",
    "import json\n",
    "\n",
    "\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import utils, HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "\n",
    "# Saves computation time, since we don't need it for the contents of this notebook\n",
    "t.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_activation_exploration.feature_detection.experiment import feature_detection_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "gpt2_small = HookedTransformer.from_pretrained(\"gpt2-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = {}\n",
    "# Specify the file path\n",
    "file_path = 'datasets/fantasy_200.json'\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "  # Load the JSON data from the file\n",
    "  dataset_fantasy = json.load(file)\n",
    "\n",
    "  stories[\"fantasy\"] = dataset_fantasy\n",
    "\n",
    "# Specify the file path\n",
    "file_path = 'datasets/scifi_200.json'\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "  # Load the JSON data from the file\n",
    "  dataset_scifi = json.load(file)\n",
    "\n",
    "  stories[\"scifi\"] = dataset_scifi\n",
    "\n",
    "# Specify the file path\n",
    "file_path = 'datasets/sports_200.json'\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "  # Load the JSON data from the file\n",
    "  dataset_sports = json.load(file)\n",
    "\n",
    "  stories[\"sports\"] = dataset_sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the baseline dataset\n",
    "\n",
    "def read_all_text_files(directory):\n",
    "    # List to hold the contents of all files\n",
    "    contents_list = []\n",
    "\n",
    "    # List all files in directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Check if file is a text file\n",
    "        if filename.endswith('.txt'):\n",
    "            # Construct full file path\n",
    "            filepath = os.path.join(directory, filename)\n",
    "\n",
    "            # Open the file and read the contents\n",
    "            with open(filepath, 'r') as f:\n",
    "                contents = f.read()\n",
    "\n",
    "            # Add the file contents to the list\n",
    "            contents_list.append(contents)\n",
    "\n",
    "    return contents_list\n",
    "\n",
    "training_subset = read_all_text_files('datasets/urlsf_subset01-1_data') + read_all_text_files('datasets/urlsf_subset01-182_data')\n",
    "\n",
    "baseline_dataset = training_subset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx be:  0\n",
      "batch_idx be:  1\n",
      "batch_idx be:  2\n",
      "batch_idx be:  3\n",
      "batch_idx be:  4\n",
      "batch_idx be:  0\n",
      "batch_idx be:  1\n",
      "batch_idx be:  2\n",
      "batch_idx be:  3\n",
      "batch_idx be:  4\n",
      "feature vector calculated\n",
      "batch_idx be:  0\n",
      "batch_idx be:  1\n",
      "batch_idx be:  2\n",
      "batch_idx be:  3\n",
      "batch_idx be:  4\n",
      "batch_idx be:  0\n",
      "batch_idx be:  1\n",
      "batch_idx be:  2\n",
      "batch_idx be:  3\n",
      "batch_idx be:  4\n",
      "part 2 done\n",
      "batch_idx be:  0\n",
      "batch_idx be:  1\n",
      "batch_idx be:  2\n",
      "batch_idx be:  3\n",
      "batch_idx be:  4\n",
      "batch_idx be:  0\n",
      "batch_idx be:  1\n",
      "batch_idx be:  2\n",
      "batch_idx be:  3\n",
      "batch_idx be:  4\n",
      "part 3 done\n",
      "Optimal threshold: 2994.5676\n",
      "Maximum accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "feature_dataset = stories[\"fantasy\"][:10] \n",
    "\n",
    "threshold_dataset = {\n",
    "    \"positive\": stories[\"fantasy\"][100:110],\n",
    "    \"negative\": stories[\"sports\"][100:110]\n",
    "}\n",
    "\n",
    "evaluation_dataset = {\n",
    "    \"positive\": stories[\"fantasy\"][10:20],\n",
    "    \"negative\": stories[\"sports\"][10:20]\n",
    "}\n",
    "\n",
    "feature_detection_classifier(\n",
    "    gpt2_small,\n",
    "    \"blocks.7.hook_resid_post\",\n",
    "    feature_dataset,\n",
    "    baseline_dataset,\n",
    "    False,\n",
    "    threshold_dataset,\n",
    "    evaluation_dataset,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tran-mask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
